{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting text from round 800 resumes \n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "directory = 'all resumes'\n",
    "\n",
    "output_file_path = 'extracted_resumes.txt'\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        \n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_reader = PdfReader(file_path)\n",
    "           \n",
    "            for pdf_page in pdf_reader.pages:\n",
    "                output_file.write(pdf_page.extract_text() )\n",
    "          \n",
    "\n",
    "        \n",
    "        elif filename.endswith('.docx'):\n",
    "            doc = Document(file_path)\n",
    "            for paragraph in doc.paragraphs:\n",
    "                output_file.write(paragraph.text)\n",
    "            \n",
    "print(output_file_path)\n",
    "print(f\"Extracted content saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Model using data that we extracted from pdfs and docx \n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "nlp = spacy.blank(\"en\")  \n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    " \n",
    "with open(\"/content/train_data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    training_data = json.load(file)\n",
    " \n",
    "def clean_overlapping_entities(entities):\n",
    "    entities.sort(key=lambda x: x[0])\n",
    "    cleaned_entities = []\n",
    "    last_end = -1\n",
    " \n",
    "    for start, end, label in entities:\n",
    "        if start < last_end:  \n",
    "            continue  \n",
    "        cleaned_entities.append((start, end, label))\n",
    "        last_end = end  \n",
    " \n",
    "    return cleaned_entities\n",
    " \n",
    " \n",
    "all_data = []\n",
    "for entry in training_data:\n",
    "    try:\n",
    "        text, annotations = entry\n",
    "        entities = annotations[\"entities\"]\n",
    "        cleaned_entities = clean_overlapping_entities(entities)\n",
    "        all_data.append((text, {\"entities\": cleaned_entities}))\n",
    "    except (ValueError, KeyError) as e:\n",
    "        print(f\"Data entry is incorrectly formatted: {entry}, Error: {e}\")\n",
    " \n",
    " \n",
    "train_data, test_data = train_test_split(all_data, test_size=0.15, random_state=42)\n",
    " \n",
    " \n",
    "print(f\"Total valid training entries: {len(train_data)}\")\n",
    "print(f\"Total valid testing entries: {len(test_data)}\")\n",
    " \n",
    " \n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    " \n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    " \n",
    "# Training the model\n",
    "n_iter = 750\n",
    "with nlp.disable_pipes(*unaffected_pipes):  \n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        losses = {}\n",
    "   \n",
    "        random.shuffle(train_data)\n",
    "   \n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in zip(texts, annotations)]\n",
    "            nlp.update(examples, drop=0.3, losses=losses)\n",
    "        print(f\"Iteration {itn + 1}/{n_iter}, Loss: {losses['ner']}\")\n",
    " \n",
    "# Specify the path in your Google Drive\n",
    "model_output_path = \"/content/drive/MyDrive/saved ner\"  \n",
    "os.makedirs(model_output_path, exist_ok=True)  \n",
    " \n",
    "#nlp.to_disk(model_output_path)\n",
    "#print(f\"Model training complete and saved to {model_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the trained model\n",
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    " \n",
    "# Load the trained model\n",
    "model_output_path = \"/content/drive/MyDrive/saved ner\"  # Adjust to your model path\n",
    "nlp = spacy.load(model_output_path)\n",
    " \n",
    "y_true = []\n",
    "y_pred = []\n",
    " \n",
    " \n",
    "for text, annotations in test_data:\n",
    "    doc = nlp(text)\n",
    " \n",
    "    \n",
    "    predicted_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "   \n",
    "\n",
    "    true_entities = [(ent[0], ent[1], ent[2]) for ent in annotations['entities']]\n",
    "   \n",
    "\n",
    "    y_true.extend([label for _, _, label in true_entities])\n",
    "    y_pred.extend([label for _, _, label in predicted_entities])\n",
    "   \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Entities: {predicted_entities}\")\n",
    "    print(f\"True Entities: {true_entities}\\n\")\n",
    " \n",
    "print(f\"true labels: {len(y_true)}\")\n",
    "print(f\"predicted labels: {len(y_pred)}\")\n",
    " \n",
    "if not y_pred:\n",
    "    print(\"No predicted labels were found. Check the model output.\")\n",
    " \n",
    "if len(y_true) == len(y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "else:\n",
    "    print(\"true != Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally checking how much our trained model is working\n",
    "import spacy\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    " \n",
    "# Load your trained NER model\n",
    "model_output_path = \"/content/drive/MyDrive/saved ner\"\n",
    "nlp = spacy.load(model_output_path)\n",
    " \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    " \n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = \"\"\n",
    "    doc = Document(docx_path)\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    " \n",
    "def extract_entities_from_text(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    " \n",
    "file_path = \"/content/SHOAIB.pdf\" # change pfd or docx accordingly\n",
    " \n",
    " \n",
    "if file_path.endswith('.pdf'):\n",
    "    extracted_text = extract_text_from_pdf(file_path)\n",
    "elif file_path.endswith('.docx'):\n",
    "    extracted_text = extract_text_from_docx(file_path)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported file type. Please provide a PDF or DOCX file.\")\n",
    " \n",
    "#print(\"Extracted Text:\")\n",
    "#print(extracted_text)\n",
    " \n",
    "entities = extract_entities_from_text(extracted_text)\n",
    "print(\"Extracted Entities:\")\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
